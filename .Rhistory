fit_d<-lm(sales~-1+c1+c2+c3+advertising)
summary(fit_d)
s
fit_c<-lm(sales~-1+c1+c2+c3+advertising)
summary(fit_c)
max(abs(fitted(fit_a)-fitted(fit_b)))
max(abs(fitted(fit_a)-fitted(fit_b)))
max(abs(fitted(fit_b)-fitted(fit_c)))
max(abs(fitted(fit_a)-fitted(fit_b)))
max(abs(fitted(fit_b)-fitted(fit_c)))
max(abs(fitted(fit_a)-fitted(fit_c)))
summary(fit_a)
summary(fit_b)
summary(fit_c)
summary(fit)
summary(fit_a)
summary(fit_b)
summary(fit_c)
# This uses the same data set as in Rcode1.R:
Advertising <- read.csv("data.Advertising.csv", header=T)
# setwd("~/0 - Teaching/Computational Statistics/Rcode")
setwd("/Users/hkromer/02_PhD/08.ETH_courses/01.CompStat/02.ETH_course.CompStat/")
getwd()
?lm               # lm stands for linear model
# This uses the same data set as in Rcode1.R:
Advertising <- read.csv("data.Advertising.csv", header=T)
fit.all <- lm(sales ~ TV + radio + newspaper, data=Advertising)
fit.all           # shows call and estimated beta's
summary(fit.all)  # shows more info
# Basic things we need:
n <- nrow(Advertising)
names(Advertising)
y <- Advertising[,5]
X <- as.matrix( cbind(1, Advertising[,c(2:4)]) )
XtX.inv <- solve(t(X) %*% X)           # (X^T X)^{-1}
# Compute estimates by hand:
(hatbeta <- XtX.inv %*% t(X) %*% y)      # hatbeta = (X^T X)^{-1} X^T y
# Compute fitted values by hand:
y.hat <- X %*% hatbeta                 # yhat = X hatbeta
# Compare to fitted.values(fit.all):
max( abs(y.hat - fitted.values(fit.all)) )
# Re-compute residuals:
res <- y - y.hat
bad <- levels(shelveloc)[1]==shelveloc
medium <- levels(shelveloc)[3]==shelveloc
good <- levels(shelveloc)[2]==shelveloc
a1 <- medium*1
a2 <- good*1
# large model: distinguishes between medium and good
fit_a<-lm(sales~a1+a2+advertising)
# small model: does not distinguish between medium and good
d1 <- bad*1
d2 <- bad*0   # not bad --> medium or good
fit_d <- lm(sales~d1+d2+advertising)
# and conducting a partial F-test:
(fit.anova <- anova(fit_d, fit_a)   )
summary(fit)
summary(fit_a)
summary(fit_b)
summary(fit_c)
airline <- scan("http://stat.ethz.ch/Teaching/Datasets/airline.dat")
airline <- scan("http://stat.ethz.ch/Teaching/Datasets/airline.dat")
?airline
airline
plot(airline)
plot(airline, xlab="Index Jan 1949-Dec1960", ylab="Monthly number of flight passengers")
# b) Compute the logarithm of the data and plot against time. Comment on the difference.
log.airline <- log(airline)
# b) Compute the logarithm of the data and plot against time. Comment on the difference.
log.airline <- log(airline)
plot(log.airline , xlab="Index Jan 1949-Dec1960", ylab="Monthly number of flight passengers")
plot(log.airline , xlab="Index Jan 1949-Dec1960", ylab="Monthly number of flight passengers (log)")
x1<-rep(c(1,rep(0,11)),12)
x1
t<-1:144
t
x1<-rep(c(1,rep(0,11)),12)
x2<-rep(c(0,1,rep(0,10)),12)
x3<-rep(c(0,0,1,rep(0,9)),12)
x4<-rep(c(0,0,0,1,rep(0,8)),12)
x5<-rep(c(0,0,0,0,1,rep(0,7)),12)
x6<-rep(c(0,0,0,0,01,rep(0,9)),12)
x6
x6<-rep(c(0,0,0,0,0,1,rep(0,9)),12)
x6
x1<-rep(c(1,rep(0,11)),12)
x2<-rep(c(0,1,rep(0,10)),12)
x3<-rep(c(0,0,1,rep(0,9)),12)
x4<-rep(c(0,0,0,1,rep(0,8)),12)
x5<-rep(c(0,0,0,0,1,rep(0,7)),12)
x6<-rep(c(0,0,0,0,0,1,rep(0,6)),12)
x7<-rep(c(rep(0,6),1,rep(0,5)),12)
x8<-rep(c(rep(0,7),1,rep(0,4)),12)
x9<-rep(c(rep(0,8),1,rep(0,3)),12)
x10<-rep(c(rep(0,9),1,rep(0,2)),12)
x11<-rep(c(rep(0,10),1,0),12)
x12<-rep(c(rep(0,11),1),12)
fit <- lm(airline.log~-1+t+x1+x2+x3+x4+x5+x6+x7+x8+x9+x10+x11+x12)
fit <- lm(log.airline~-1+t+x1+x2+x3+x4+x5+x6+x7+x8+x9+x10+x11+x12)
(s <- summary(fit))
# fitted values
yhat <- fitted.values(fit)
# residuals
et <-residuals(fit)
plot(t, yhat)
lines(t, yhat)
plot(log.airline , xlab="Index Jan 1949-Dec1960", ylab="Monthly number of flight passengers (log)")
lines(t, yhat, col="red")
plot(log.airline , xlab="Index Jan 1949-Dec1960", ylab="Monthly number of flight passengers (log)")
lines(t, yhat, col=c(2))
legend("topleft", c("data", "fitted values"), col=c(1,2), pch=20)
plot(t, et, main="Residuals vs time")
plot(fit, which=c(1))
plot(t, et, main="Residuals vs time")
lines(t, mean(et), col=c(2))
plot(log.airline , xlab="Index Jan 1949-Dec1960", ylab="Monthly number of flight passengers (log)")
lines(t, yhat, col=c(2))
s
exp(0.0100688*12)
0.0100688*12
exp(0.0100688*10)
exp(0.0100688*1)
airline <- scan("http://stat.ethz.ch/Teaching/Datasets/airline.dat")
# a) Plot the data against time and describe what you observe.
plot(airline, xlab="Index Jan 1949-Dec1960", ylab="Monthly number of flight passengers")
# b) Compute the logarithm of the data and plot against time. Comment on the difference.
log.airline <- log(airline)
plot(log.airline , xlab="Index Jan 1949-Dec1960", ylab="Monthly number of flight passengers (log)")
x1<-rep(c(1,rep(0,11)),12)
x2<-rep(c(0,1,rep(0,10)),12)
x3<-rep(c(0,0,1,rep(0,9)),12)
x4<-rep(c(0,0,0,1,rep(0,8)),12)
x5<-rep(c(0,0,0,0,1,rep(0,7)),12)
x6<-rep(c(0,0,0,0,0,1,rep(0,6)),12)
x7<-rep(c(rep(0,6),1,rep(0,5)),12)
x8<-rep(c(rep(0,7),1,rep(0,4)),12)
x9<-rep(c(rep(0,8),1,rep(0,3)),12)
x10<-rep(c(rep(0,9),1,rep(0,2)),12)
x11<-rep(c(rep(0,10),1,0),12)
x12<-rep(c(rep(0,11),1),12)
t<-1:144
fit <- lm(log.airline~-1+t+x1+x2+x3+x4+x5+x6+x7+x8+x9+x10+x11+x12)
(s <- summary(fit))
# fitted values
yhat <- fitted.values(fit)
# residuals
et <-residuals(fit)
plot(log.airline , xlab="Index Jan 1949-Dec1960", ylab="Monthly number of flight passengers (log)")
lines(t, yhat, col=c(2))
legend("topleft", c("data", "fitted values"), col=c(1,2), pch=20)
plot(t, et, main="Residuals vs time")
# I do not think that the model assumptions hold. For low and high values of t the residuals are small
# e) Give an interpretation of the parameter beta in the above model if we consider the original scale.
# e) Give an interpretation of the parameter beta in the above model if we consider the original scale.
exp(12*0.0100688)
#twelve indicators x1, · · · , x12 encoding the month.
s1 <- rep(c(rep(0,2),rep(1,3),rep(0,7)),12) # spring
s2 <- rep(c(rep(0,5),rep(1,3),rep(0,4)),12) # summer
s3 <- rep(c(rep(0,8),rep(1,3),0),12) # herbst
s4 <- rep(c(1,1,rep(0,9),1),12) # summer
s4
fit.seasons <- lm(log.airline~-1+t+s1+s2+s3+s4)
(s.seasons <- summary(fit.seasons))
# and conducting a partial F-test:
(fit.anova <- anova(fit.seasons, fit) )
fit.seasons <- lm(log.airline~-1+t+s1+s2+s3+s4)
(s.seasons <- summary(fit.seasons))
# fitted values
yhat <- fitted.values(fit.seasons)
plot(log.airline , xlab="Index Jan 1949-Dec1960", ylab="Monthly number of flight passengers (log)")
lines(t, yhat, col=c(2))
# and conducting a partial F-test:
(fit.anova <- anova(fit.seasons, fit) )
anova(fit,fit.seasons)
(s.seasons <- summary(fit.seasons))
# fitted values
yhat <- fitted.values(fit.seasons)
plot(log.airline , xlab="Index Jan 1949-Dec1960", ylab="Monthly number of flight passengers (log)")
lines(t, yhat, col=c(4))
# and conducting a partial F-test:
(fit.anova <- anova(fit.seasons, fit) )
# Conduct a partial F-test to check whether we can use four predictors indicating the seasons
# s1 , · · · , s4 (s1 for spring (month 3,4,5),. . . , s4 for winter (month 12,1,2)) instead of
#twelve indicators x1, · · · , x12 encoding the month.
s1 <- rep(c(rep(0,6),rep(1,6)),12) # spring
s2 <- rep(c(rep(1,6),rep(0,6)),12) # spring
fit.seasons2 <- lm(log.airline~-1+t+s1+s2)
(s.seasons2 <- summary(fit.seasons2))
# fitted values
yhat2 <- fitted.values(fit.seasons2)
lines(t, yhat2, col=c(4))
# and conducting a partial F-test:
(fit.anova <- anova(fit.seasons2, fit.seasons) )
# Conduct a partial F-test to check whether we can use four predictors indicating the seasons
# s1 , · · · , s4 (s1 for spring (month 3,4,5),. . . , s4 for winter (month 12,1,2)) instead of
#twelve indicators x1, · · · , x12 encoding the month.
s1 <- rep(c(rep(0,2),rep(1,3),rep(0,7)),12) # spring
s2 <- rep(c(rep(0,5),rep(1,3),rep(0,4)),12) # summer
s3 <- rep(c(rep(0,8),rep(1,3),0),12) # herbst
s4 <- rep(c(1,1,rep(0,9),1),12) # summer
fit.seasons <- lm(log.airline~-1+t+s1+s2+s3+s4)
(s.seasons <- summary(fit.seasons))
# fitted values
yhat <- fitted.values(fit.seasons)
# residuals
et <-residuals(fit)
plot(log.airline , xlab="Index Jan 1949-Dec1960", ylab="Monthly number of flight passengers (log)")
lines(t, yhat, col=c(4))
plot(t, et, main="Residuals vs time")
# residuals
et <-residuals(fit.seasons)
plot(log.airline , xlab="Index Jan 1949-Dec1960", ylab="Monthly number of flight passengers (log)")
lines(t, yhat, col=c(4))
plot(t, et, main="Residuals vs time")
# and conducting a partial F-test:
(fit.anova <- anova(fit.seasons, fit) )
plot(log.airline , xlab="Index Jan 1949-Dec1960", ylab="Monthly number of flight passengers (log)")
lines(t, yhat, col=c(4))
# setwd("~/0 - Teaching/Computational Statistics/Rcode")
setwd("/Users/hkromer/02_PhD/08.ETH_courses/01.CompStat/02.ETH_course.CompStat/")
getwd()
##################################################################
# Fitting linear models in R
?lm               # lm stands for linear model
# This uses the same data set as in Rcode1.R:
Advertising <- read.csv("data.Advertising.csv", header=T)
fit.all <- lm(sales ~ TV + radio + newspaper, data=Advertising)
fit.all           # shows call and estimated beta's
# Interpretation:
#   predicted sales = 2.94 + 0.046*TV + 0.189*Radio - 0.001*Newspaper
summary(fit.all)  # shows more info
# Let's understand all this output and recompute it "by hand"
# Basic things we need:
n <- nrow(Advertising)
names(Advertising)
y <- Advertising[,5]
X <- as.matrix( cbind(1, Advertising[,c(2:4)]) )
XtX.inv <- solve(t(X) %*% X)           # (X^T X)^{-1}
# Compute estimates by hand:
(hatbeta <- XtX.inv %*% t(X) %*% y)      # hatbeta = (X^T X)^{-1} X^T y
# Compare to summary(fit.all)
# Compute fitted values by hand:
y.hat <- X %*% hatbeta                 # yhat = X hatbeta
# Compare to fitted.values(fit.all):
max( abs(y.hat - fitted.values(fit.all)) )
# Re-compute residuals:
res <- y - y.hat
# Compare to given residuals:
max(abs(res-residuals(fit.all)))
# Re-compute summary statistics of residuals
summary(res, digits=3) # Note some left skewness
# Compare to summary(fit.all)
# Re-compute residual standard error (RSE):
p <- 4                                # intercept and three variables
sum(res^2)/(n-p)                      # sigmahat^2 = RSS/(n-p)
(RSE <- sqrt( sum(res^2)/(n-p) ))     # RSE = sqrt(RSS/(n-p))
# sigmahat^2 is a measure of goodness of fit.
# It is an estimate of sigma^2, the variance of the statistical errors
# The smaller the number, the better the fit (points closer to the line)
# The RSE is measured in the same units as the dependent variable.
# Plot
plot(residuals(fit.all), ylim=c(-6,6), main="Residuals")
# In case of normally distributed errors, we expect:
#   About 66% of the points are within +/- hat.sigma
#     from the regression plane (blue dotted lines)
#   Aboute 95% of the points are within +/- 2*hat.sigma
#     from the regression plane (orange dotted lines)
abline(h=0, lty=2)
abline(h=1.686, lty=3, col="blue", lwd=2)
abline(h=-1.686, lty=3, col="blue", lwd=2)
abline(h=2*1.686, lty=3, col="orange", lwd=2)
abline(h=-2*1.686, lty=3, col="orange", lwd=2)
# Re-compute R^2:
RSS <- sum( (y-y.hat)^2 )
TSS <- sum( (y-mean(y))^2 )
(Rsquared <- 1 - RSS/TSS)
# Interpretation: proportion of variance explained by regression model
# Re-compute adjusted R^2:
(Rsquared.adj <- 1 - (RSS/(n-p))/(TSS/(n-1)))
# Adjusted for number of variables in the model
# Diagnostic plots
plot(fit.all, which=c(1,2))  # Tukey Anscombe plot and QQ plot of residuals
##################################################################
### Be careful with interpretation of coefficients in multiple regression!
# We illustrate this in a small simulation
n  <- 10000             # use large sample size to get precise estimates
x1 <- rnorm(n,0,1)
x2 <- 1.5*x1 + rnorm(n,0, 0.1)
y  <- 2*x1 - 3*x2 + rnorm(n,0,0.1)
# Fit different regression models:
coef(lm(y~x1))       # in this model, beta1 = -2.5
coef(lm(y~x2))       # in this model, beta2 = -1.67
coef(lm(y~x1+x2))    # in this model, beta1 = +2.0 and beta2 = -3.0
# Note: The sign of beta1 flipped between the 1st and 3rd model.
#       The value of beta2 changed between the 2nd and 3rd model.
# This shows that the interpretation of each beta_k depends on
#   the other variables in the model!
# Correct interpretation of beta_k in model Y~x_1+...+x_p:
# Comparing two observations i and j,
#   where x_{ik} = x_{jk} + 1 (i.e., values for x_k differ by one)
#   and x_{ir} = x_{jr} for all other r in {1,..,p}
#   (i.e., values for all other variables x_r are identical),
#   then it follows by plugging in that E(Y_i) = E(Y_j) + beta_k.
# So beta_k can be interpreted as the expected difference in y values,
#   E(Y_i)-E(Y_j), for two observations i and j, where x_{ik} = x_{jk} + 1
#   and x_{ir} = x_{jr} for all other r in {1,..,p}.
# You also see in the formulation where the other variables come in.
# You can do the same for the fitted values, using ^y and ^beta:
# Comparing two observations i and j,
#   where x_{ik} = x_{jk} + 1 (i.e., values for x_k differ by one)
#   and x_{ir} = x_{jr} for all other r in {1,..,p}
#   (i.e., values for all other variables x_r are identical),
#   then by plugging in it follows that ^y_i = ^y_j + ^beta_k.
# So ^beta_k can be interpreted as the difference in fitted y values,
#   ^y_i - ^y_j, for two observations i and j, where x_{ik} = x_{jk} + 1
#   and x_{ir} = x_{jr} for all other r in {1,..,p}.
# 3d visualization
library(rgl)
n  <- 100                     # small sample size for plot
x1 <- rnorm(n,0,1)
x2 <- 1.5*x1 + rnorm(n,0,.8)  # bit more variance for plot
y  <- 2*x1 - 3*x2 + rnorm(n,0,.5)
ind <- order(x2)
plot3d(x1[ind],x2[ind],y[ind], col=rainbow(n), xlab="x1", ylab="x2",zlab="y")
fit <- lm(y ~ x1+x2)
planes3d(a=coef(fit)[2], b=coef(fit)[3], c=-1, d=coef(fit)[1], alpha=.5)
# The points are colored accoding to their x2 value.
# Looking at all points, there is a negative relationship between x1 and y.
#   This corresponds to the negative coefficient of x1 in y~x1.
# Looking at the points of each color separately (i.e., holding x2 fixed),
#   there is a positive relationship between x1 and y.
#   This corresponds to the positive coefficient of x1 in y~x1+x2.
###################################################################
# Understand individual p-values:
# Re-compute standard error for TV:
se.TV <- RSE * sqrt(XtX.inv[2,2])
se.TV
# Reproduce t-value for TV:
tval.TV <- hatbeta[2] / se.TV
tval.TV
# Compute p-value for TV:
2*pt(abs(tval.TV), df=n-p, lower=FALSE)
# This is so small that we cannot check it.
# Compute p-value for Newspaper:
2*pt(0.177, df=n-p,lower=FALSE)
# We can also reproduce the p-value for Newspaper by comparing two models:
fit.TV.radio <- lm(sales ~ TV + radio, data=Advertising) # leaving out newspaper
# and conducting a partial F-test:
anova(fit.TV.radio, fit.all)
# The p-value depends on the other variables in the model:
fit.Newsp <- lm(sales ~ newspaper, data=Advertising)
summary(fit.Newsp)
# Now newspaper is significant.
# How can you explain this?
# Reproduce p-value again by comparing two models:
fit.empty <- lm(sales ~ 1, data=Advertising)
anova(fit.empty,fit.Newsp)
################################################################
# Overall F-test
# P-value at bottom right is from overall F-test
# This compares the full model against the empty model
anova(fit.empty,fit.all)
# Good to look at this first, before looking at individual p-values
#   This avoids multiple testing problems.
########################################################################
### Example
football <- read.table("http://www.statsci.org/data/general/punting.txt", header = TRUE)
head(football)
dim(football)
names(football)
pairs(football[,c(1,3,4)])
fit <- lm(Distance ~ R_Strength + L_Strength, data=football)
summary(fit)
# The individual p-values are all insignificant,
#   whereas the p-value of the overall F-test is highly significant.
# How can you explain this?
########################################################################
# Categorical variables
# Example: Credit data
library(ISLR)
data(Credit)
?Credit
summary(Credit)
pairs(Credit[,c(2:7,12)])
attach(Credit)
# student is a categorical variable with two levels
fit.student <- lm(Balance ~ Income + Student, data=Credit)
summary(fit.student)
# plot results:
coeff <- coefficients(fit.student)
plot(Income, Balance, col=Student, pch=20, main="Credit data")
legend("topleft", c("No student", "Student"), col=c(1,2), pch=20)
abline(a=coeff[1], b=coeff[2], col=1, lwd=2)
abline(a=coeff[1]+coeff[3], b=coeff[2], col=2, lwd=2)
# ethnicity is a categorical variable with more than two levels
fit.ethn <- lm(Balance ~ Income + Ethnicity, data=Credit)
summary(fit.ethn)
# plot results:
coeff <- coefficients(fit.ethn)
plot(Income, Balance, col=Ethnicity, pch=20, main="Credit data")
legend("topleft", c("Afr-Amer", "Asian", "Caucasian"), col=c(1,2,3), pch=20)
abline(a=coeff[1], b=coeff[2], col=1, lwd=2)
abline(a=coeff[1]+coeff[3], b=coeff[2], col=2, lwd=2)
abline(a=coeff[1]+coeff[4], b=coeff[2], col=3, lwd=2)
# It looks like ethnicity is not important for predicting Balance
#   if we already use income.
# Can we test this?
# What p-value should we look at?
# Conduct partial F-test again:
fit.income <- lm(Balance ~ Income, data=Credit)
anova(fit.income, fit.ethn)
###################################################################
# Interaction: example R code
fit.interact <- lm(Balance ~ Income + Student + Income*Student, data=Credit)
summary(fit.interact)
coeff <- coef(fit.interact)
plot(Income, Balance, col=Student, pch=20, main="Credit data")
legend("topleft", c("No student", "Student"), col=c(1,2), pch=20)
abline(a=coeff[1], b=coeff[2], col=1, lwd=2)
abline(a=coeff[1]+coeff[3], b=coeff[2]+coeff[4], col=2, lwd=2)
# Interaction between Income and Student status means that:
# The "effect" of Income depends on student status
#   in the sense that slopes are different for students and non-students
# The "effect" of Student status depends on Income
#   in the sense that vertical distance between the lines depends on Income level
# One can test if the interaction is significant using a partial F test
# (do this yourself)
# setwd("~/0 - Teaching/Computational Statistics/Rcode")
setwd("/Users/hkromer/02_PhD/08.ETH_courses/01.CompStat/02.ETH_course.CompStat/")
getwd()
?lm               # lm stands for linear model
# This uses the same data set as in Rcode1.R:
Advertising <- read.csv("data.Advertising.csv", header=T)
fit.all <- lm(sales ~ TV + radio + newspaper, data=Advertising)
fit.all           # shows call and estimated beta's
summary(fit.all)  # shows more info
# Basic things we need:
n <- nrow(Advertising)
names(Advertising)
y <- Advertising[,5]
X <- as.matrix( cbind(1, Advertising[,c(2:4)]) )
XtX.inv <- solve(t(X) %*% X)           # (X^T X)^{-1}
# Compute estimates by hand:
(hatbeta <- XtX.inv %*% t(X) %*% y)      # hatbeta = (X^T X)^{-1} X^T y
# Compute fitted values by hand:
y.hat <- X %*% hatbeta                 # yhat = X hatbeta
# Compare to fitted.values(fit.all):
max( abs(y.hat - fitted.values(fit.all)) )
# Re-compute residuals:
res <- y - y.hat
# Compare to given residuals:
max(abs(res-residuals(fit.all)))
# Re-compute summary statistics of residuals
summary(res, digits=3) # Note some left skewness
# Re-compute residual standard error (RSE):
p <- 4                                # intercept and three variables
sum(res^2)/(n-p)                      # sigmahat^2 = RSS/(n-p)
(RSE <- sqrt( sum(res^2)/(n-p) ))     # RSE = sqrt(RSS/(n-p))
# Plot
plot(residuals(fit.all), ylim=c(-6,6), main="Residuals")
# In case of normally distributed errors, we expect:
#   About 66% of the points are within +/- hat.sigma
#     from the regression plane (blue dotted lines)
#   Aboute 95% of the points are within +/- 2*hat.sigma
#     from the regression plane (orange dotted lines)
abline(h=0, lty=2)
abline(h=1.686, lty=3, col="blue", lwd=2)
abline(h=-1.686, lty=3, col="blue", lwd=2)
abline(h=2*1.686, lty=3, col="orange", lwd=2)
abline(h=-2*1.686, lty=3, col="orange", lwd=2)
# Re-compute R^2:
RSS <- sum( (y-y.hat)^2 )
TSS <- sum( (y-mean(y))^2 )
(Rsquared <- 1 - RSS/TSS)
# Re-compute adjusted R^2:
(Rsquared.adj <- 1 - (RSS/(n-p))/(TSS/(n-1)))
# Diagnostic plots
plot(fit.all, which=c(1,2))  # Tukey Anscombe plot and QQ plot of residuals
x1 <- rnorm(n,0,1)
x2 <- 1.5*x1 + rnorm(n,0, 0.1)
y  <- 2*x1 - 3*x2 + rnorm(n,0,0.1)
# Fit different regression models:
coef(lm(y~x1))       # in this model, beta1 = -2.5
coef(lm(y~x2))       # in this model, beta2 = -1.67
coef(lm(y~x1+x2))    # in this model, beta1 = +2.0 and beta2 = -3.0
# 3d visualization
library(rgl)
n  <- 100                     # small sample size for plot
x1 <- rnorm(n,0,1)
x2 <- 1.5*x1 + rnorm(n,0,.8)  # bit more variance for plot
y  <- 2*x1 - 3*x2 + rnorm(n,0,.5)
ind <- order(x2)
plot3d(x1[ind],x2[ind],y[ind], col=rainbow(n), xlab="x1", ylab="x2",zlab="y")
fit <- lm(y ~ x1+x2)
planes3d(a=coef(fit)[2], b=coef(fit)[3], c=-1, d=coef(fit)[1], alpha=.5)
# Re-compute standard error for TV:
se.TV <- RSE * sqrt(XtX.inv[2,2])
se.TV
# Reproduce t-value for TV:
tval.TV <- hatbeta[2] / se.TV
tval.TV
# Compute p-value for TV:
2*pt(abs(tval.TV), df=n-p, lower=FALSE)
# Compute p-value for Newspaper:
2*pt(0.177, df=n-p,lower=FALSE)
# We can also reproduce the p-value for Newspaper by comparing two models:
fit.TV.radio <- lm(sales ~ TV + radio, data=Advertising) # leaving out newspaper
# and conducting a partial F-test:
anova(fit.TV.radio, fit.all)
# The p-value depends on the other variables in the model:
fit.Newsp <- lm(sales ~ newspaper, data=Advertising)
summary(fit.Newsp)
